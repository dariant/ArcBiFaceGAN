{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Prepare the training dataset:\n",
    "To prepare the dataset of face images follow the structure found in `DATASETS/example_dataset`. The dataset should contain a `VIS` directory with visible spectrum images, and a `NIR` directory with corresponding near-infrared images. \n",
    "Images should use the naming convention `{identity}_{sample_name}.jpg`. Corresponding images in the `VIS` and `NIR` directories should share the same name. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Add identity features to the training dataset:\n",
    "To create identity features of the training images use the script `create_training_identity_features.py`. The identity features are saved in the `identity_features.json` file in the dataset directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"DATASETS/example_dataset\"\n",
    "recognition_model = \"Arcface_files/ArcFace_r100_ms1mv3_backbone.pth\"\n",
    "\n",
    "!bash ./docker_run.sh python create_training_identity_features.py --data_folder=$data_folder --model=$recognition_model --gpu=0 --all_or_one=\"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script relies on the following arguments: \n",
    "* `--rec_model` should point to the `.pth` file of a pretrained recognition model\n",
    "* `--gpu_device_number` determines which GPU to use (e.g. `--gpu_device_number=0`)\n",
    "*  `--all_or_one`  determines whether to use identity features of each image in the dataset (`all`) or one most representative identity feature per identity (`one`)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Train the identity-conditioned StyleGAN2 model:\n",
    "To train the identity-conditioned StyleGAN2 of ArcBiFaceGAN use the `training.py` script as follows:   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path_to_training_dataset = \"DATASETS/example_dataset\"\n",
    "output_folder = \"EXPERIMENTS/training_output\"\n",
    "\n",
    "NIR_loss_weight = 0.1\n",
    "\n",
    "!bash ./docker_run.sh python training.py --NIR_loss_weight=$NIR_loss_weight --cfg=\"auto\" --snap=20 --gpus=1 --mirror=1 --gpu_device_number=0 --batch=12  --data=$path_to_training_dataset  --outdir=$output_folder #--cond=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script relies on the following arguments: \n",
    "* `--data` should point to the training dataset with `VIS` and `NIR` subdirectories\n",
    "* `--outdir` determines the output directory\n",
    "* `--NIR_loss_weight` defines the weight of the NIR Discriminator in the final loss calculation\n",
    "* `--cfg` determines the model configuration (e.g. number of blocks, image resolution)\n",
    "* `--snap` defines the frequency of snapshots during training\n",
    "* `--batch` determines the batch size\n",
    "* `--mirror=1` enables horizontal flipping of training images\n",
    "* `--gpu_device_number` determines which GPU to use, if you want to use one\n",
    "* `--gpus` determines the amount of available GPUs, if you want to use multiple (only works in certain environments)\n",
    "* `--cond=0` can be used to disable training based on the identity condition\n",
    "\n",
    "For details on other possible arguments and available configurations check the [StyleGAN2-ADA](https://github.com/NVlabs/stylegan2-ada-pytorch) documentation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.5. Continue training with updates NIR loss weight:\n",
    "To continue training from a saved checkpoint use the `--resume` argument, i.e. `--resume={path_to_pretrained_model}`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_training_dataset = \"DATASETS/example_dataset\"\n",
    "path_to_pretrained_pkl_model = \"\"\n",
    "output_folder = \"EXPERIMENTS/training_output_continued\"\n",
    "\n",
    "NIR_loss_weight = 0.5\n",
    "\n",
    "!bash ./docker_run.sh python training.py --NIR_loss_weight=$NIR_loss_weight --cfg=\"auto\" --snap=20 --gpus=1 --mirror=1 --GPU_DEVICE_NUMBER=0 --batch=12  --data=$path_to_training_dataset --resume=$path_to_pretrained_pkl_model --outdir=$output_folder --cond=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Generate synthetic recognition datasets:\n",
    "To generate data using ArcBiFaceGAN use the `generate_recognition_data.py` script as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_pretrained_pkl_model = \"\"\n",
    "recognition_model = \"Arcface_files/ArcFace_r100_ms1mv3_backbone.pth\"\n",
    "output_folder = \"EXPERIMENTS/synthetic_output_example\"\n",
    "path_to_training_identity_features=\"DATASETS/example_dataset/identity_features.json\" \n",
    "ids = 100\n",
    "samples_per_id = 32\n",
    "seed = 0\n",
    "gpu_device_number = 0\n",
    "\n",
    "!bash ./docker_run.sh python generate_recognition_data.py --gen_model=$path_to_pretrained_pkl_model --rec_model=$recognition_model --outdir=$output_folder --training_ids=$path_to_training_identity_features --ids=$ids --samples_per_id=$samples_per_id --seed=$seed --gpu_device_number=$gpu_device_number  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script relies on the following arguments: \n",
    "* `--gen_model` should point to the `.pkl` file of the identity-conditioned StyleGAN2 model that was trained in the previous step\n",
    "* `--rec_model` should point to the `.pth` file of the pretrained recognition model to be used for filtering\n",
    "* `--training_ids` should point to the  `.json` file of training identity features (i.e. identities of real-world subjects)\n",
    "*  `--outdir` determines the output directory\n",
    "* `--ids` defines the amount of synthetic identities to be generated\n",
    "* `--samples_per_id` controls the amount of samples to be generated per synthetic identity\n",
    "* `--seed` determines which starting seed to use \n",
    "* `--truncation` controls the truncation factor of the latent space (see the [StyleGAN2-ADA](https://github.com/NVlabs/stylegan2-ada-pytorch) documentation)\n",
    "*  `--gpu_device_number` determines which GPU device to use (e.g. `0` or `1`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "18479fac5746d32b43421ca5e0ee2dd5840b086e4ab03684aa8ec83129c0c31d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('dataGAN': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
